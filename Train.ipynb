{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.activations import *\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import itertools\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "import math\n",
    "from utils import lex\n",
    "from utils import yacc\n",
    "from utils import cpp\n",
    "import zipfile\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Configure The Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_name = \"./data.zip\"\n",
    "\n",
    "# The collumns containing the code info\n",
    "cols = [\"code\", \"block\"]\n",
    "# The Archive Containing The Actual Codes\n",
    "archive = zipfile.ZipFile('data.zip', 'r')\n",
    "# The first name is the name of the containing folder of the codes \n",
    "list_files = archive.namelist()[1:]\n",
    "# Now We will make a scanner for the C++ language\n",
    "scanner = lex.lex(cpp)\n",
    "# If any thing was considered fault at the line i, we will consider all the lines [i - range_n, i + range_n) to be fault \n",
    "range_n = 4\n",
    "# Then We Define The literals of the program\n",
    "lits = cpp.literals\n",
    "# Then We Define The Tokens\n",
    "toks = list(cpp.tokens)\n",
    "# We remove the White Space token to add it later \n",
    "toks.remove(\"CPP_WS\")\n",
    "# We add the White Space token here because we want it to have the value of zero, we'll use this latter for padding lines of code\n",
    "toks.insert(0, \"CPP_WS\")\n",
    "# Tok 2 N : a dictionary from tokens to thier integer, mapped, value\n",
    "tok2n = dict(zip(toks + [i for i in lits], itertools.count()))\n",
    "# N 2 Tok : a dictionary from integers to thier token, mapped, value\n",
    "n2tok = dict(zip(itertools.count(), toks + [i for i in lits]))\n",
    "\n",
    "# The maximum value we allow in as a constant value in a code\n",
    "max_v = 2147483647 - 1\n",
    "\n",
    "# The amount of importance we give to 1s 0s and false postives and false negatives\n",
    "WEIGHTS_FOR_LOSS = np.array([[2,0.5],[0.1,0.1]])\n",
    "\n",
    "\n",
    "NN_MODEL = \"nn\"\n",
    "RANDOM_FOREST_MODEL = \"RF\"\n",
    "KNN_MODEL = \"KNN\"\n",
    "\n",
    "# The type of model we will use in our work\n",
    "model_type = RANDOM_FOREST_MODEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Make A Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(weights, rnn=True):\n",
    "        \n",
    "    '''\n",
    "    gives us the loss function\n",
    "    '''\n",
    "    def w_categorical_crossentropy_mine(y_true, y_pred):\n",
    "        nb_cl = len(weights)\n",
    "        \n",
    "        if(not rnn):\n",
    "            final_mask = K.zeros_like(y_pred[:, 0])\n",
    "            y_pred_max = K.max(y_pred, axis=1)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, c_p] * K.cast(y_true, tf.float32)[:, c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "        else:\n",
    "            final_mask = K.zeros_like(y_pred[:, :,0])\n",
    "            y_pred_max = K.max(y_pred, axis=2)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], K.shape(y_pred)[1], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, :,c_p] * K.cast(y_true, tf.float32)[:, :,c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "\n",
    "            \n",
    "    return w_categorical_crossentropy_mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Data From The Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(list_files, archive, log = False):\n",
    "    '''\n",
    "    reads the data and handles the range_n number\n",
    "    '''\n",
    "    res = []\n",
    "    for i in list_files:\n",
    "        try :\n",
    "            x = pd.read_csv(archive.open(i), sep = \"`\")\n",
    "            res.append(x)\n",
    "        except Exception: \n",
    "            print(i)\n",
    "            continue\n",
    "    resF = []\n",
    "    for n_i, i in enumerate(res) :\n",
    "        \n",
    "        if i.shape[0] is 0 :\n",
    "            continue\n",
    "            \n",
    "        a = i.values\n",
    "        b = a.copy()\n",
    "#         This line of code will change the data from (Features, beingWrong) to (Features, beginRight, beingWrong)\n",
    "        b = np.concatenate([b[:, :-1], b[:, -1:].astype(np.int) ^ 1, b[:, -1:]], axis = -1)\n",
    "        \n",
    "        \n",
    "        for j in range(len(b)):\n",
    "            if np.sum(a[j - range_n : j + range_n, -1]) > 0 :\n",
    "#                 This was explained before the declaration of range_n\n",
    "                b[j, -1] = 1\n",
    "                b[j, -2] = 0\n",
    "        for x in range(len(b)):\n",
    "            for y in range(len(b[x])):\n",
    "#                 Here we will try to change any thing that is not the code it self and which is a string into numbers \n",
    "                if y > 1 :\n",
    "                    if type(b[x, y]) == str :\n",
    "                        try :\n",
    "                            float(b[x, y].strip())\n",
    "                        except Exception : \n",
    "                            b[x, y] = -3\n",
    "                elif y == 1 :\n",
    "                    b[x, y] = \"DATA DOES NOT MATTER\"\n",
    "#         By 0s we mean the code being fine and so on\n",
    "        b = pd.DataFrame(b, columns=list(i.columns)[:-1] + [\"0s\", \"1s\"])\n",
    "        b.replace(\"#empty\", np.nan, inplace =True)\n",
    "        resF.append(b.dropna())\n",
    "        \n",
    "    if log :     \n",
    "        print(\"data was read and changed\")    \n",
    "    return resF\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Scanner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement(scanner, string_in):\n",
    "    \n",
    "    '''\n",
    "    gets a string and returns the None, 2 which is the tokenized version\n",
    "    '''\n",
    "    try :\n",
    "        scanner.input(string_in)\n",
    "    except Exception as e :\n",
    "        print(\"Exception in using the lex\", e)\n",
    "        print(string_in)\n",
    "    token = scanner.token()\n",
    "    \n",
    "    \n",
    "#     id2n and n2id are the same as n2tok tok2n but they are extended to contain the information of the symbol table of each code separately\n",
    "    id2n = dict(zip([i for i in lits], [tok2n[i] for i in lits]))\n",
    "    n2id = dict(zip([tok2n[i] for i in lits], [i for i in lits]))\n",
    "    \n",
    "    n_id = len(lits) + 1\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    while token is not None :\n",
    "        \n",
    "        t = token.type\n",
    "        \n",
    "#         If we have recieved a token and it is not something we need to use ord for\n",
    "        if t in cpp.tokens :\n",
    "#             Reciving a white space\n",
    "            if token.type == cpp.tokens[cpp.tokens.index(\"CPP_WS\")]:\n",
    "                #this is because this will make it easier for us to pad our data\n",
    "                v = 0\n",
    "#             Reciving an ID from the code\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_ID\")]:\n",
    "                v = token.value\n",
    "#                 Checking if need to add the id to n2id or not\n",
    "                if v in id2n.keys() :\n",
    "                    pass\n",
    "                else :\n",
    "                    id2n[v] = n_id\n",
    "                    n2id[n_id] = v\n",
    "                    \n",
    "                    n_id += 1\n",
    "                v = id2n[v]\n",
    "#             If we receive a string (We don't use the value of strings)\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_STRING\")]:\n",
    "                v = -1\n",
    "#             If we recive #\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_POUND\")]:\n",
    "                v = -2\n",
    "#             If we recive ##\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_DPOUND\")]:\n",
    "                v = -3\n",
    "#             If we recive char\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_CHAR\")]:\n",
    "                v = -4\n",
    "            elif token.type in cpp.tokens[3:]:\n",
    "                print(\"some thing went really wrong\")\n",
    "#             Parsing the value of constant values\n",
    "            else:\n",
    "                try :\n",
    "                    tv = token.value.lower()\n",
    "                    if tv[-1] == \"l\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if tv[-1] == \"u\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if \"x\" in  tv :\n",
    "                        v = int(tv, base = 16)\n",
    "                    elif tv[-1].lower() == \"l\":\n",
    "                        if tv[-2].lower() == \"u\" :\n",
    "                            v = float(tv[:-2])\n",
    "                        else :\n",
    "                            v = float(tv[:-1])\n",
    "                    else :\n",
    "                        v = float(tv)\n",
    "                    v = np.clip(v, - max_v, max_v)\n",
    "                    \n",
    "                except Exception as e :\n",
    "                    print(\"Couldn't scan this number\", token)\n",
    "                    return\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        else :\n",
    "            v = ord(t)\n",
    "        try :\n",
    "            t = tok2n[t]\n",
    "        except Exception :\n",
    "            n = len(id2n.keys()) + 1 \n",
    "            tok2n[t] = n\n",
    "            n2tok[n] = t\n",
    "            id2n[t] = n\n",
    "            n2id[n] = t\n",
    "            t = tok2n[t]\n",
    "            \n",
    "        res.append([t, v])\n",
    "        token = scanner.token()\n",
    "        \n",
    "    res = np.array(res)\n",
    "    \n",
    "    return res\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Tokenize Our Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    \n",
    "    '''\n",
    "    reads data and tokenizes each of the sentences and adds them together.\n",
    "    The out put will contain the actual data, max number of lines per code and mean number of lines per code\n",
    "    the actual data will have the following shape :\n",
    "    \n",
    "    Number of codes, Number of lines per each code , 2 (Data and State)\n",
    "    State will contain (Code being right, Code Being Wrong)\n",
    "    Data Will Contain (Number Of Words, 2 (Token, Value))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    res = []\n",
    "    x = []\n",
    "    mean = 0\n",
    "    max_num = 0\n",
    "    for i in data:\n",
    "#         If We had any code submissions that was empty, we skip them\n",
    "        if i.shape[0] == 0 :\n",
    "            continue \n",
    "        temp = []\n",
    "        mean += i.shape[0]\n",
    "        max_num = max(max_num, i.shape[0])\n",
    "        \n",
    "        for j in i.values :\n",
    "            \n",
    "            try :\n",
    "                tok = get_replacement(scanner, j[0]).astype(np.float32)\n",
    "            except Exception as e :\n",
    "                continue\n",
    "                \n",
    "            x.append(tok)\n",
    "            \n",
    "            y = j[-2:]\n",
    "            temp.append([tok, y])\n",
    "            \n",
    "        res.append(temp)\n",
    "    mean /= len(res)\n",
    "    \n",
    "    return res, mean, max_num\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding The Data In The Statement Level (Adding Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cols(num, res, empty):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pads or removes data so they all have the same shape in one code  file \n",
    "    \n",
    "    num : amount of word we'll have per each line\n",
    "    empty : what we'll use to pad our data with\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    resF = []\n",
    "    \n",
    "    for i in res :\n",
    "        \n",
    "        temp = []\n",
    "        \n",
    "        if (len(i) == 0):\n",
    "#           We'll any coding file which is empty\n",
    "            continue \n",
    "            \n",
    "        for j in i :\n",
    "            \n",
    "#             J[0] is the data and J[1] is the state\n",
    "\n",
    "            if len(j[0]) < num :\n",
    "                \n",
    "                result = np.concatenate([j[0], np.ones(( num - len(j[0]), 2)) * empty], axis = 0)\n",
    "                \n",
    "            elif len(j[0]) > num :\n",
    "                result = j[0][:num, :]\n",
    "            else :\n",
    "                result = j[0]\n",
    "                \n",
    "            result = result.reshape((-1))\n",
    "            \n",
    "#             This is so that we'll have the data and our state at the same time\n",
    "            result = np.concatenate([result, np.array([j[1]]).reshape((-1))], axis = 0)\n",
    "            \n",
    "            temp.append(np.array(result))\n",
    "        \n",
    "        resF.append(np.array(temp))\n",
    "        \n",
    "    resF = np.array(resF)\n",
    "    \n",
    "    return resF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating The 32 Processed Columns And Lexical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_data(tokenized_final, data):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    adds the information from the parser to the things that were gained from the information of scanners\n",
    "    tokenized_final will be the output of \"change_cols\" and data will be the output of \"get_data\"\n",
    "    '''\n",
    "#     The first line reads data and drops the following columns : columns containing text of the parser or lex and the \n",
    "#     The last two columns which are the state of the code which we are trying to predict\n",
    "    dataR = np.concatenate([i.drop(cols, axis = 1).values[:, :-2] for i in data], axis = 0)\n",
    "    dataR = dataR.astype(np.float32)\n",
    "    \n",
    "    cnt = 0 \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    \n",
    "    for i in tokenized_final : \n",
    "        temp = []\n",
    "        for j in i :\n",
    "            \n",
    "            add = dataR[cnt, :]\n",
    "            temp.append(np.concatenate([add, j], axis = 0))\n",
    "            \n",
    "            cnt += 1\n",
    "            \n",
    "            \n",
    "        res.append(np.array(temp))\n",
    "    res = np.array(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using All The Functions Above And Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(list_data, archive, scaler = None, add_all = False, type_add = 0,\n",
    "                pad1 = None, pad2 = None, return_before_pad = False, cons_per_line = 10, log  = False):\n",
    "    '''\n",
    "    Reading, Tokenizing, Concatenating And Normalizing Data\n",
    "    list_data : Name Of The Codes We Are Using\n",
    "    scalar : Scalar Used To Normalize Data, If None Is Presented, The Function Will Compute One\n",
    "    add_all : Whether Or Not We Want All Our Codes To Have The Same Amount As For The Lines Of Code\n",
    "    type_add : The amount of lines each code should contain : {0 : mean, 1 : max number of lines}\n",
    "    pad1 : The amount of words each line should contain, If None is presented const_per_line + mean(amount of words per line) would be used\n",
    "    pad2 : The amount of lines each code should contain, type_add would not be used if pad2 is not None\n",
    "    return_before_pad : Whether or not to also return the data before it was padded to have the same amount of lines percode \n",
    "    '''\n",
    "#     First We Read Our Data From The Zip File\n",
    "    data = get_data(list_data, archive)\n",
    "#     The We tokenize our data\n",
    "    r, mean, max_num = tokenize_data(data)\n",
    "    \n",
    "#     Then We Create Our Empty Vector\n",
    "    empty = np.array([tok2n[\"CPP_WS\"], 0]).reshape(1, 2).astype(np.float32)\n",
    "    \n",
    "#     The Defualt Option for Padding \n",
    "    if pad1 is None :\n",
    "        pad1 = int(mean) + cons_per_line\n",
    "        \n",
    "#     We Padd Our Data At Each Line With Extra White Spaces\n",
    "    res = change_cols(pad1, r, empty)\n",
    "    r = np.array(res)\n",
    "    \n",
    "#     Here we will concatenate our lexical features and our preprocessed features\n",
    "    r = get_final_data(r, data)\n",
    "    if log :\n",
    "        print(\"Padded The Lexical And Preprocessed Features of Data\")\n",
    "    \n",
    "    \n",
    "    res = r\n",
    "    \n",
    "    if add_all :\n",
    "        \n",
    "        if log :\n",
    "            print(\"Computing How Many Empty Line To Add To Codes \")\n",
    "        if pad2 is None :\n",
    "            \n",
    "            mean = 0\n",
    "            max_num = -1\n",
    "            for i in r :\n",
    "                mean += i.shape[0]\n",
    "                max_num = max(max_num, i.shape[0])\n",
    "                \n",
    "                \n",
    "            mean /= r.shape[0]\n",
    "            nums = [int(mean), max_num]\n",
    "            pad2 = nums[type_add]\n",
    "            \n",
    "        res = []\n",
    "        if log :\n",
    "            print(\"Computed How Many Empty Line To Add To Codes \")\n",
    "        for i in r :\n",
    "            \n",
    "            if i.shape[0] < pad2 :\n",
    "                \n",
    "                zeros = np.zeros([pad2 - i.shape[0], i.shape[1]])\n",
    "                zeros[:, -2] = 1\n",
    "                temp = np.concatenate([i, zeros], axis = 0)\n",
    "            elif i.shape[0] > pad2 :\n",
    "                temp = i[:pad2, :]\n",
    "            else :\n",
    "                temp = i\n",
    "            res.append(temp)\n",
    "        if log :\n",
    "            print(\"Added All The Empty Lines \")\n",
    "\n",
    "    res = np.array(res)\n",
    "    \n",
    "    save_r = r.copy()\n",
    "    \n",
    "    r = np.concatenate(res, axis = 0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    if scaler is None :\n",
    "        \n",
    "        scaler = StandardScaler().fit(r[:, :-2].astype(np.float32))\n",
    "        if log :\n",
    "            print(\"Computed Mean And Standard Deviation For Normalization \")\n",
    "    \n",
    "    for i, iv in enumerate(res) :\n",
    "        \n",
    "        res[i, :, :-2] = scaler.transform(iv[:, :-2].astype(np.float32)).astype(np.float32)\n",
    "        \n",
    "        if log :\n",
    "            print(\"Data Was Normalized \")\n",
    "\n",
    "    \n",
    "    if return_before_pad :\n",
    "        return res, scaler, pad1, pad2, save_r\n",
    "        \n",
    "    return res, scaler, pad1, pad2\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                 \n",
    "                \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    gets the first rnn model\n",
    "    shape : shape of the input : shape of the codes [Number of lines (which can be None), Number of Fetures per line]\n",
    "    '''\n",
    "    in1 = Input(shape)\n",
    "    X = Bidirectional(LSTM(150, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(in1)\n",
    "    X = LSTM(150, return_sequences=True, dropout=0.25, recurrent_dropout=0.1,)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(256, activation=relu)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(128, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Dense(64, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(32, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(16, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    X = Dense(2, activation=softmax)(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The Ranodm Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_model(shape, k):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_jobs=-1)\n",
    "    tuned_parameters =  {\"n_estimators\" : [100 * (i*2) for i in range(1, 4)],\n",
    "                         \"max_depth\" : [2**i for i in range(5, 8)], \n",
    "                         \"min_samples_split\" : list(np.linspace(.2, .8, 4)),\n",
    "                         \"min_samples_leaf\" : list(np.linspace(.2, .5, 4)),\n",
    "#                          \"max_features\" : [shape[-1] // (2**i) for i in range(1)]\n",
    "                         \n",
    "                        }\n",
    "    res = GridSearchCV(rf, tuned_parameters, n_jobs=-1, cv=k)\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The K-NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_KNN_model(shape, k):\n",
    "    \n",
    "    rf = KNeighborsClassifier(n_jobs=-1)\n",
    "    tuned_parameters =  {\n",
    "                         \"n_neighbors\" : [min(int(shape[0]*((k-1)/k)), 10 * (i*2)) for i in range(1, 11)],\n",
    "                        }\n",
    "    res = GridSearchCV(rf, tuned_parameters, n_jobs=-1, cv=k)\n",
    "    \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Measurements For Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_true, y_pred):\n",
    "    \n",
    "    acc = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "#     True Positive\n",
    "    tp =  acc[1][1]\n",
    "#     False Negative\n",
    "    fn =  acc[1][0]\n",
    "#     False Positive\n",
    "    fp =  acc[0][1]\n",
    "#     True Negative\n",
    "    tn =  acc[0][0]\n",
    "#     Recall\n",
    "    rec1 = acc[1][1] / (acc[1][1] + acc[1][0])\n",
    "#     Precision \n",
    "    prec1 = acc[1][1] / (acc[1][1] + acc[0][1])\n",
    "#     Over ALl Accuracy\n",
    "    accuracy = (acc[1][1] + acc[0][0]) / (acc[1][1] + acc[0][0] + acc[1][0] + acc[0][1])\n",
    "#     F1 Accuracy\n",
    "    f1 = 2.0 / ((1.0/rec1) + (1.0/prec1))\n",
    "    \n",
    "    return rec1, prec1, accuracy, f1 , tp , fn , fp , tn\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Paring Data For Measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mus(y_true, x, model):\n",
    "    \n",
    "    global model_type\n",
    "    \n",
    "    if model_type == NN_MODEL:\n",
    "        y_t = np.argmax(y_true, axis = -1).reshape((-1))\n",
    "        y_p = model.predict(x)\n",
    "        y_p = np.argmax(y_p, axis=-1).reshape((-1))\n",
    "    else : \n",
    "        y_t = y_true\n",
    "        y_p = model.predict(x)\n",
    "\n",
    "    \n",
    "    return get_acc(y_t, y_p)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using All The Defined Functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 0\n",
      "data read\n",
      "training on the data started \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on the data finished \n",
      "train : rec1, prec1, accuracy, f1, acc , tp , fn , fp , tn \n",
      "(0.0, nan, 0.935672514619883, nan, 0, 11, 0, 160)\n",
      "test : rec1, prec1, accuracy, f1, acc, tp , fn , fp , tn \n",
      "(nan, nan, 1.0, nan, 0, 0, 0, 19)\n",
      "k 1\n",
      "data read\n",
      "training on the data started \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5092ab0fe84d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32melse\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training on the data finished \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    686\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    665\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 667\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    552\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    553\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\poori\\.conda\\envs\\myonlyenv\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We should know the type of the model inorder to know what to do at runtime for data preparation and also model compilation \n",
    "assert(model_type is not None)\n",
    "\n",
    "\n",
    "# K is for our K-fold\n",
    "k = 10\n",
    "# The name of the codes we want to use, you can slice this list to a smaller list for a fast test\n",
    "l = list_files[:]\n",
    "# The number of codes we use on each fold\n",
    "size = math.ceil(len(l) / k)\n",
    "# Verbose for our NN model \n",
    "verbose = 0 \n",
    "\n",
    "# The results for trains and tests respectivly\n",
    "trs = []\n",
    "ts = []\n",
    "\n",
    "for i in range(k):\n",
    "    \n",
    "    print(\"k\", i)\n",
    "#     start and end will be the indicies for what we'll use for test\n",
    "    start  = i * size\n",
    "    end    = min(len(l), (i + 1) * size)\n",
    "    \n",
    "    data_train = l[:start] + l[end:]\n",
    "    data_test  = l[start : end]\n",
    "    \n",
    "    if len(data_test) <= 0 or len(data_train) <= 0 :\n",
    "        print(\"Empty Data Set Found\")\n",
    "        continue\n",
    "\n",
    "   \n",
    "    # gathering data for train\n",
    "    r_train, scaler, pad1, pad2 = gather_data(data_train, archive, add_all = True)\n",
    "    # gathering data for test, please note that the same mean and standard deviation that was computed for train will be used to \n",
    "    # normalize test data and also the information of pad1 and pad2 is computed from train so that no information will be \n",
    "    # leaked from train and also none of the aforementioned are dependent on the test data \n",
    "    r_test, _, _, _ = gather_data(data_test, archive, scaler = scaler, add_all = True, pad1 = pad1, pad2 = pad2)\n",
    "    \n",
    "    print(\"data read\")\n",
    "    \n",
    "    \n",
    "    # making the X, y for train and test set\n",
    "    X_train = r_train[:, :, :-2]\n",
    "    y_train = r_train[:, :, -2:]\n",
    "    \n",
    "    \n",
    "    X_test = r_test[:, :, :-2]\n",
    "    y_test = r_test[:, :, -2:]\n",
    "    \n",
    "    \n",
    "    if not (model_type == NN_MODEL) :\n",
    "        \n",
    "        X_train = X_train[:].reshape((-1, X_train.shape[-1]))\n",
    "        y_train = np.argmax(y_train[:], -1).reshape((-1))\n",
    "        X_test = X_test[:].reshape((-1, X_train.shape[-1]))\n",
    "        y_test = np.argmax(y_test[:], -1).reshape((-1))\n",
    "    \n",
    "    \n",
    "    # configuring model\n",
    "    if model_type == NN_MODEL : \n",
    "        model = get_model([None, r_train.shape[-1] - 2])\n",
    "        loss = get_loss_function(WEIGHTS_FOR_LOSS)\n",
    "        model.compile(keras.optimizers.adam(lr = 1e-3), keras.losses.categorical_crossentropy, metrics = [\"accuracy\"])\n",
    "    elif model_type == RANDOM_FOREST_MODEL : \n",
    "        model = get_random_forest_model(X_train.shape, k)\n",
    "    elif model_type == KNN_MODEL : \n",
    "        model = get_KNN_model(X_train.shape, k)\n",
    "    else :\n",
    "        raise Exception(\"You didn't use the right command for the model\")\n",
    "    \n",
    "    # training the model\n",
    "    print(\"training on the data started \")\n",
    "    \n",
    "    if model_type == NN_MODEL : \n",
    "        model.fit(X_train, y_train, validation_data = [X_test, y_test], epochs = 20, batch_size = 8, verbose = verbose)\n",
    "    else : \n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"training on the data finished \")\n",
    "    \n",
    "    # saving and printing the accuracy of training data\n",
    "    print(\"train : rec1, prec1, accuracy, f1, acc , tp , fn , fp , tn \")\n",
    "    trs.append(get_mus(y_train, X_train,model))\n",
    "    print(trs[-1])\n",
    "\n",
    "    # preparing to write the training accuracy to a file\n",
    "    strRes = \"train : \"\n",
    "    for counter in range(8):\n",
    "        strRes = strRes + '%.5f' % trs[-1][counter] + \" , \"\n",
    "    \n",
    "    strRes += \" \\n \"\n",
    "    \n",
    "\n",
    "    # saving and printing the accuracy of testing data\n",
    "    print(\"test : rec1, prec1, accuracy, f1, acc, tp , fn , fp , tn \")\n",
    "    ts.append(get_mus(y_test, X_test,model))\n",
    "    print(ts[-1])\n",
    "    \n",
    "    # preparing to write the testing accuracy to a file\n",
    "    strRes += \" test :  \"\n",
    "    for counter in range(8):\n",
    "        strRes = strRes + '%.5f' % ts[-1][counter] + \" , \"\n",
    "    \n",
    "    # writing the accuracies on to a file\n",
    "    f = open(\"test.txt\", \"a\")\n",
    "    f.write(strRes + \"\\n\")\n",
    "    f.close()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trs = np.array(trs)\n",
    "ts = np.array(ts)\n",
    "print(\"avg train : rec1, prec1, accuracy, f1, acc\")\n",
    "print(np.mean(trs[:, : 4], axis=0))\n",
    "print(\"avg test : rec1, prec1, accuracy, f1, acc\")\n",
    "print(np.mean(ts[:, : 4], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
