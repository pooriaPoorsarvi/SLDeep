{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.activations import *\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import itertools\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "import math\n",
    "from utils import lex\n",
    "from utils import yacc\n",
    "from utils import cpp\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Configure The Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_name = \"./data.zip\"\n",
    "\n",
    "\n",
    "cols = [\"code\", \"block\"]\n",
    "# The first name is the name of the containing folder of the codes \n",
    "archive = zipfile.ZipFile('data.zip', 'r')\n",
    "list_files = archive.namelist()[1:]\n",
    "# Now We will make a scanner for the C++ language\n",
    "scanner = lex.lex(cpp)\n",
    "# If any thing was considered fault at the line i, we will consider all the lines [i - range_n, i + range_n) to be fault \n",
    "range_n = 4\n",
    "# Then We Define The literals of the program\n",
    "lits = cpp.literals\n",
    "# Then We Define The Tokens\n",
    "toks = list(cpp.tokens)\n",
    "# We remove the White Space token to add it later \n",
    "toks.remove(\"CPP_WS\")\n",
    "# We add the White Space token here because we want it to have the value of zero, we'll use this latter for padding lines of code\n",
    "toks.insert(0, \"CPP_WS\")\n",
    "# Tok 2 N : a dictionary from tokens to thier integer, mapped, value\n",
    "tok2n = dict(zip(toks + [i for i in lits], itertools.count()))\n",
    "# N 2 Tok : a dictionary from integers to thier token, mapped, value\n",
    "n2tok = dict(zip(itertools.count(), toks + [i for i in lits]))\n",
    "\n",
    "# The maximum value we allow in as a constant value in a code\n",
    "max_v = 2147483647 - 1\n",
    "\n",
    "# The amount of \n",
    "WEIGHTS_FOR_LOSS = np.array([[2,0.5],[0.1,0.1]])\n",
    "\n",
    "cons_per_line = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Make A Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(weights, rnn=True):\n",
    "        \n",
    "    '''\n",
    "    gives us the loss function\n",
    "    '''\n",
    "    def w_categorical_crossentropy_mine(y_true, y_pred):\n",
    "        nb_cl = len(weights)\n",
    "        \n",
    "        if(not rnn):\n",
    "            final_mask = K.zeros_like(y_pred[:, 0])\n",
    "            y_pred_max = K.max(y_pred, axis=1)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, c_p] * K.cast(y_true, tf.float32)[:, c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "        else:\n",
    "            final_mask = K.zeros_like(y_pred[:, :,0])\n",
    "            y_pred_max = K.max(y_pred, axis=2)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], K.shape(y_pred)[1], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, :,c_p] * K.cast(y_true, tf.float32)[:, :,c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "\n",
    "            \n",
    "    return w_categorical_crossentropy_mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Data From The Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(list_files, archive):\n",
    "    '''\n",
    "    reads the data and handles the range_n number\n",
    "    '''\n",
    "    res = []\n",
    "    for i in list_files:\n",
    "        try :\n",
    "            x = pd.read_csv(archive.open(i), sep = \"`\")\n",
    "            x = x[x.columns[:-1]]\n",
    "            res.append(x)\n",
    "        except Exception: \n",
    "            print(i)\n",
    "            continue\n",
    "    resF = []\n",
    "    for n_i, i in enumerate(res) :\n",
    "        \n",
    "        if i.shape[0] is 0 :\n",
    "            continue\n",
    "            \n",
    "        a = i.values\n",
    "        b = a.copy()\n",
    "#         This line of code will change the data from (Features, beingWrong) to (Features, beginRight, beingWrong)\n",
    "        b = np.concatenate([b[:, :-1], b[:, -1:].astype(np.int) ^ 1, b[:, -1:]], axis = -1)\n",
    "        \n",
    "        \n",
    "        for j in range(len(b)):\n",
    "            if np.sum(a[j - range_n : j + range_n, -1]) > 0 :\n",
    "#                 This was explained before the declaration of range_n\n",
    "                b[j, -1] = 1\n",
    "                b[j, -2] = 0\n",
    "        for x in range(len(b)):\n",
    "            for y in range(len(b[x])):\n",
    "#                 Here we will try to change any thing that is not the code it self and which is a string into numbers \n",
    "                if y > 1 :\n",
    "                    if type(b[x, y]) == str :\n",
    "                        try :\n",
    "                            float(b[x, y].strip())\n",
    "                        except Exception : \n",
    "                            b[x, y] = -3\n",
    "                elif y == 1 :\n",
    "                    b[x, y] = \"DATA DOES NOT MATTER\"\n",
    "#         By 0s we mean the code being fine and so on\n",
    "        b = pd.DataFrame(b, columns=list(i.columns)[:ind_2] + [\"0s\", \"1s\"])\n",
    "        b.replace(\"#empty\", np.nan, inplace =True)\n",
    "        resF.append(b.dropna())\n",
    "        \n",
    "        \n",
    "    print(\"data was read and changed\")    \n",
    "    return resF\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Scanner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement(scanner, string_in):\n",
    "    \n",
    "    '''\n",
    "    gets a string and returns the None, 2 which is the tokenized version\n",
    "    '''\n",
    "    try :\n",
    "        scanner.input(string_in)\n",
    "    except Exception as e :\n",
    "        print(\"Exception in using the lex\", e)\n",
    "        print(string_in)\n",
    "    token = scanner.token()\n",
    "    \n",
    "    \n",
    "#     id2n and n2id are the same as n2tok tok2n but they are extended to contain the information of the symbol table of each code separately\n",
    "    id2n = dict(zip([i for i in lits], [tok2n[i] for i in lits]))\n",
    "    n2id = dict(zip([tok2n[i] for i in lits], [i for i in lits]))\n",
    "    \n",
    "    n_id = len(lits) + 1\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    while token is not None :\n",
    "        \n",
    "        t = token.type\n",
    "        \n",
    "#         If we have recieved a token and it is not something we need to use ord for\n",
    "        if t in cpp.tokens :\n",
    "#             Reciving a white space\n",
    "            if token.type == cpp.tokens[cpp.tokens.index(\"CPP_WS\")]:\n",
    "                #this is because this will make it easier for us to pad our data\n",
    "                v = 0\n",
    "#             Reciving an ID from the code\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_ID\")]:\n",
    "                v = token.value\n",
    "#                 Checking if need to add the id to n2id or not\n",
    "                if v in id2n.keys() :\n",
    "                    pass\n",
    "                else :\n",
    "                    id2n[v] = n_id\n",
    "                    n2id[n_id] = v\n",
    "                    \n",
    "                    n_id += 1\n",
    "                v = id2n[v]\n",
    "#             If we receive a string (We don't use the value of strings)\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_STRING\")]:\n",
    "                v = -1\n",
    "#             If we recive #\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_POUND\")]:\n",
    "                v = -2\n",
    "#             If we recive ##\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_DPOUND\")]:\n",
    "                v = -3\n",
    "#             If we recive char\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_CHAR\")]:\n",
    "                v = -4\n",
    "            elif token.type in cpp.tokens[3:]:\n",
    "                print(\"some thing went really wrong\")\n",
    "#             Parsing the value of constant values\n",
    "            else:\n",
    "                try :\n",
    "                    tv = token.value.lower()\n",
    "                    if tv[-1] == \"l\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if tv[-1] == \"u\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if \"x\" in  tv :\n",
    "                        v = int(tv, base = 16)\n",
    "                    elif tv[-1].lower() == \"l\":\n",
    "                        if tv[-2].lower() == \"u\" :\n",
    "                            v = float(tv[:-2])\n",
    "                        else :\n",
    "                            v = float(tv[:-1])\n",
    "                    else :\n",
    "                        v = float(tv)\n",
    "                    v = np.clip(v, - max_v, max_v)\n",
    "                    \n",
    "                except Exception as e :\n",
    "                    print(\"Couldn't scan this number\", token)\n",
    "                    return\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        else :\n",
    "            v = ord(t)\n",
    "        try :\n",
    "            t = tok2n[t]\n",
    "        except Exception :\n",
    "            n = len(id2n.keys()) + 1 \n",
    "            tok2n[t] = n\n",
    "            n2tok[n] = t\n",
    "            id2n[t] = n\n",
    "            n2id[n] = t\n",
    "            t = tok2n[t]\n",
    "            \n",
    "        res.append([t, v])\n",
    "        token = scanner.token()\n",
    "        \n",
    "    res = np.array(res)\n",
    "    \n",
    "    return res\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
