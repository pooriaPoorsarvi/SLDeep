{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras \n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.activations import *\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import itertools\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "import math\n",
    "from utils import lex\n",
    "from utils import yacc\n",
    "from utils import cpp\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Configure The Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_name = \"./data.zip\"\n",
    "\n",
    "# The collumns containing the code info\n",
    "cols = [\"code\", \"block\"]\n",
    "# The Archive Containing The Actual Codes\n",
    "archive = zipfile.ZipFile('data.zip', 'r')\n",
    "# The first name is the name of the containing folder of the codes \n",
    "list_files = archive.namelist()[1:]\n",
    "# Now We will make a scanner for the C++ language\n",
    "scanner = lex.lex(cpp)\n",
    "# If any thing was considered fault at the line i, we will consider all the lines [i - range_n, i + range_n) to be fault \n",
    "range_n = 4\n",
    "# Then We Define The literals of the program\n",
    "lits = cpp.literals\n",
    "# Then We Define The Tokens\n",
    "toks = list(cpp.tokens)\n",
    "# We remove the White Space token to add it later \n",
    "toks.remove(\"CPP_WS\")\n",
    "# We add the White Space token here because we want it to have the value of zero, we'll use this latter for padding lines of code\n",
    "toks.insert(0, \"CPP_WS\")\n",
    "# Tok 2 N : a dictionary from tokens to thier integer, mapped, value\n",
    "tok2n = dict(zip(toks + [i for i in lits], itertools.count()))\n",
    "# N 2 Tok : a dictionary from integers to thier token, mapped, value\n",
    "n2tok = dict(zip(itertools.count(), toks + [i for i in lits]))\n",
    "\n",
    "# The maximum value we allow in as a constant value in a code\n",
    "max_v = 2147483647 - 1\n",
    "\n",
    "# The amount of importance we give to 1s 0s and false postives and false negatives\n",
    "WEIGHTS_FOR_LOSS = np.array([[2,0.5],[0.1,0.1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Make A Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(weights, rnn=True):\n",
    "        \n",
    "    '''\n",
    "    gives us the loss function\n",
    "    '''\n",
    "    def w_categorical_crossentropy_mine(y_true, y_pred):\n",
    "        nb_cl = len(weights)\n",
    "        \n",
    "        if(not rnn):\n",
    "            final_mask = K.zeros_like(y_pred[:, 0])\n",
    "            y_pred_max = K.max(y_pred, axis=1)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, c_p] * K.cast(y_true, tf.float32)[:, c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "        else:\n",
    "            final_mask = K.zeros_like(y_pred[:, :,0])\n",
    "            y_pred_max = K.max(y_pred, axis=2)\n",
    "            y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], K.shape(y_pred)[1], 1))\n",
    "            y_pred_max_mat = K.equal(y_pred, y_pred_max)\n",
    "            for c_p, c_t in product(range(nb_cl), range(nb_cl)):\n",
    "                final_mask += ( weights[c_t, c_p] * K.cast(y_pred_max_mat, tf.float32)[:, :,c_p] * K.cast(y_true, tf.float32)[:, :,c_t]  )\n",
    "            return K.categorical_crossentropy(y_true, y_pred, True) * final_mask \n",
    "\n",
    "            \n",
    "    return w_categorical_crossentropy_mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading The Data From The Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(list_files, archive, log = False):\n",
    "    '''\n",
    "    reads the data and handles the range_n number\n",
    "    '''\n",
    "    res = []\n",
    "    for i in list_files:\n",
    "        try :\n",
    "            x = pd.read_csv(archive.open(i), sep = \"`\")\n",
    "            x = x[x.columns[:-1]]\n",
    "            res.append(x)\n",
    "        except Exception: \n",
    "            print(i)\n",
    "            continue\n",
    "    resF = []\n",
    "    for n_i, i in enumerate(res) :\n",
    "        \n",
    "        if i.shape[0] is 0 :\n",
    "            continue\n",
    "            \n",
    "        a = i.values\n",
    "        b = a.copy()\n",
    "#         This line of code will change the data from (Features, beingWrong) to (Features, beginRight, beingWrong)\n",
    "        b = np.concatenate([b[:, :-1], b[:, -1:].astype(np.int) ^ 1, b[:, -1:]], axis = -1)\n",
    "        \n",
    "        \n",
    "        for j in range(len(b)):\n",
    "            if np.sum(a[j - range_n : j + range_n, -1]) > 0 :\n",
    "#                 This was explained before the declaration of range_n\n",
    "                b[j, -1] = 1\n",
    "                b[j, -2] = 0\n",
    "        for x in range(len(b)):\n",
    "            for y in range(len(b[x])):\n",
    "#                 Here we will try to change any thing that is not the code it self and which is a string into numbers \n",
    "                if y > 1 :\n",
    "                    if type(b[x, y]) == str :\n",
    "                        try :\n",
    "                            float(b[x, y].strip())\n",
    "                        except Exception : \n",
    "                            b[x, y] = -3\n",
    "                elif y == 1 :\n",
    "                    b[x, y] = \"DATA DOES NOT MATTER\"\n",
    "#         By 0s we mean the code being fine and so on\n",
    "        b = pd.DataFrame(b, columns=list(i.columns)[:-1] + [\"0s\", \"1s\"])\n",
    "        b.replace(\"#empty\", np.nan, inplace =True)\n",
    "        resF.append(b.dropna())\n",
    "        \n",
    "    if log :     \n",
    "        print(\"data was read and changed\")    \n",
    "    return resF\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Scanner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replacement(scanner, string_in):\n",
    "    \n",
    "    '''\n",
    "    gets a string and returns the None, 2 which is the tokenized version\n",
    "    '''\n",
    "    try :\n",
    "        scanner.input(string_in)\n",
    "    except Exception as e :\n",
    "        print(\"Exception in using the lex\", e)\n",
    "        print(string_in)\n",
    "    token = scanner.token()\n",
    "    \n",
    "    \n",
    "#     id2n and n2id are the same as n2tok tok2n but they are extended to contain the information of the symbol table of each code separately\n",
    "    id2n = dict(zip([i for i in lits], [tok2n[i] for i in lits]))\n",
    "    n2id = dict(zip([tok2n[i] for i in lits], [i for i in lits]))\n",
    "    \n",
    "    n_id = len(lits) + 1\n",
    "    \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    while token is not None :\n",
    "        \n",
    "        t = token.type\n",
    "        \n",
    "#         If we have recieved a token and it is not something we need to use ord for\n",
    "        if t in cpp.tokens :\n",
    "#             Reciving a white space\n",
    "            if token.type == cpp.tokens[cpp.tokens.index(\"CPP_WS\")]:\n",
    "                #this is because this will make it easier for us to pad our data\n",
    "                v = 0\n",
    "#             Reciving an ID from the code\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_ID\")]:\n",
    "                v = token.value\n",
    "#                 Checking if need to add the id to n2id or not\n",
    "                if v in id2n.keys() :\n",
    "                    pass\n",
    "                else :\n",
    "                    id2n[v] = n_id\n",
    "                    n2id[n_id] = v\n",
    "                    \n",
    "                    n_id += 1\n",
    "                v = id2n[v]\n",
    "#             If we receive a string (We don't use the value of strings)\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_STRING\")]:\n",
    "                v = -1\n",
    "#             If we recive #\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_POUND\")]:\n",
    "                v = -2\n",
    "#             If we recive ##\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_DPOUND\")]:\n",
    "                v = -3\n",
    "#             If we recive char\n",
    "            elif token.type == cpp.tokens[cpp.tokens.index(\"CPP_CHAR\")]:\n",
    "                v = -4\n",
    "            elif token.type in cpp.tokens[3:]:\n",
    "                print(\"some thing went really wrong\")\n",
    "#             Parsing the value of constant values\n",
    "            else:\n",
    "                try :\n",
    "                    tv = token.value.lower()\n",
    "                    if tv[-1] == \"l\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if tv[-1] == \"u\" : \n",
    "                        tv = tv[:-1]\n",
    "                    if \"x\" in  tv :\n",
    "                        v = int(tv, base = 16)\n",
    "                    elif tv[-1].lower() == \"l\":\n",
    "                        if tv[-2].lower() == \"u\" :\n",
    "                            v = float(tv[:-2])\n",
    "                        else :\n",
    "                            v = float(tv[:-1])\n",
    "                    else :\n",
    "                        v = float(tv)\n",
    "                    v = np.clip(v, - max_v, max_v)\n",
    "                    \n",
    "                except Exception as e :\n",
    "                    print(\"Couldn't scan this number\", token)\n",
    "                    return\n",
    "                \n",
    "                \n",
    "            \n",
    "            \n",
    "        else :\n",
    "            v = ord(t)\n",
    "        try :\n",
    "            t = tok2n[t]\n",
    "        except Exception :\n",
    "            n = len(id2n.keys()) + 1 \n",
    "            tok2n[t] = n\n",
    "            n2tok[n] = t\n",
    "            id2n[t] = n\n",
    "            n2id[n] = t\n",
    "            t = tok2n[t]\n",
    "            \n",
    "        res.append([t, v])\n",
    "        token = scanner.token()\n",
    "        \n",
    "    res = np.array(res)\n",
    "    \n",
    "    return res\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here We Will Tokenize Our Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(data):\n",
    "    \n",
    "    '''\n",
    "    reads data and tokenizes each of the sentences and adds them together.\n",
    "    The out put will contain the actual data, max number of lines per code and mean number of lines per code\n",
    "    the actual data will have the following shape :\n",
    "    \n",
    "    Number of codes, Number of lines per each code , 2 (Data and State)\n",
    "    State will contain (Code being right, Code Being Wrong)\n",
    "    Data Will Contain (Number Of Words, 2 (Token, Value))\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    res = []\n",
    "    x = []\n",
    "    mean = 0\n",
    "    max_num = 0\n",
    "    for i in data:\n",
    "#         If We had any code submissions that was empty, we skip them\n",
    "        if i.shape[0] == 0 :\n",
    "            continue \n",
    "        temp = []\n",
    "        mean += i.shape[0]\n",
    "        max_num = max(max_num, i.shape[0])\n",
    "        \n",
    "        for j in i.values :\n",
    "            \n",
    "            try :\n",
    "                tok = get_replacement(scanner, j[0]).astype(np.float32)\n",
    "            except Exception as e :\n",
    "                continue\n",
    "                \n",
    "            x.append(tok)\n",
    "            \n",
    "            y = j[-2:]\n",
    "            temp.append([tok, y])\n",
    "            \n",
    "        res.append(temp)\n",
    "    mean /= len(res)\n",
    "    \n",
    "    return res, mean, max_num\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding The Data In The Statement Level (Adding Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_cols(num, res, empty):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    pads or removes data so they all have the same shape in one code  file \n",
    "    \n",
    "    num : amount of word we'll have per each line\n",
    "    empty : what we'll use to pad our data with\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    resF = []\n",
    "    \n",
    "    for i in res :\n",
    "        \n",
    "        temp = []\n",
    "        \n",
    "        if (len(i) == 0):\n",
    "#           We'll any coding file which is empty\n",
    "            continue \n",
    "            \n",
    "        for j in i :\n",
    "            \n",
    "#             J[0] is the data and J[1] is the state\n",
    "\n",
    "            if len(j[0]) < num :\n",
    "                \n",
    "                result = np.concatenate([j[0], np.ones(( num - len(j[0]), 2)) * empty], axis = 0)\n",
    "                \n",
    "            elif len(j[0]) > num :\n",
    "                result = j[0][:num, :]\n",
    "            else :\n",
    "                result = j[0]\n",
    "                \n",
    "            result = result.reshape((-1))\n",
    "            \n",
    "#             This is so that we'll have the data and our state at the same time\n",
    "            result = np.concatenate([result, np.array([j[1]]).reshape((-1))], axis = 0)\n",
    "            \n",
    "            temp.append(np.array(result))\n",
    "        \n",
    "        resF.append(np.array(temp))\n",
    "        \n",
    "    resF = np.array(resF)\n",
    "    \n",
    "    return resF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating The 32 Processed Columns And Lexical Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_data(tokenized_final, data):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    adds the information from the parser to the things that were gained from the information of scanners\n",
    "    tokenized_final will be the output of \"change_cols\" and data will be the output of \"get_data\"\n",
    "    '''\n",
    "#     The first line reads data and drops the following columns : columns containing text of the parser or lex and the \n",
    "#     The last two columns which are the state of the code which we are trying to predict\n",
    "    dataR = np.concatenate([i.drop(cols, axis = 1).values[:, :-2] for i in data], axis = 0)\n",
    "    dataR = dataR.astype(np.float32)\n",
    "    \n",
    "    cnt = 0 \n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    \n",
    "    for i in tokenized_final : \n",
    "        temp = []\n",
    "        for j in i :\n",
    "            \n",
    "            add = dataR[cnt, :]\n",
    "            temp.append(np.concatenate([add, j], axis = 0))\n",
    "            \n",
    "            cnt += 1\n",
    "            \n",
    "            \n",
    "        res.append(np.array(temp))\n",
    "    res = np.array(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using All The Functions Above And Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(list_data, archive, scaler = None, add_all = False, type_add = 0,\n",
    "                pad1 = None, pad2 = None, return_before_pad = False, cons_per_line = 10, log  = False):\n",
    "    '''\n",
    "    Reading, Tokenizing, Concatenating And Normalizing Data\n",
    "    list_data : Name Of The Codes We Are Using\n",
    "    scalar : Scalar Used To Normalize Data, If None Is Presented, The Function Will Compute One\n",
    "    add_all : Whether Or Not We Want All Our Codes To Have The Same Amount As For The Lines Of Code\n",
    "    type_add : The amount of lines each code should contain : {0 : mean, 1 : max number of lines}\n",
    "    pad1 : The amount of words each line should contain, If None is presented const_per_line + mean(amount of words per line) would be used\n",
    "    pad2 : The amount of lines each code should contain, type_add would not be used if pad2 is not None\n",
    "    return_before_pad : Whether or not to also return the data before it was padded to have the same amount of lines percode \n",
    "    '''\n",
    "#     First We Read Our Data From The Zip File\n",
    "    data = get_data(list_data, archive)\n",
    "#     The We tokenize our data\n",
    "    r, mean, max_num = tokenize_data(data)\n",
    "    \n",
    "#     Then We Create Our Empty Vector\n",
    "    empty = np.array([tok2n[\"CPP_WS\"], 0]).reshape(1, 2).astype(np.float32)\n",
    "    \n",
    "#     The Defualt Option for Padding \n",
    "    if pad1 is None :\n",
    "        pad1 = int(mean) + cons_per_line\n",
    "        \n",
    "#     We Padd Our Data At Each Line With Extra White Spaces\n",
    "    res = change_cols(pad1, r, empty)\n",
    "    r = np.array(res)\n",
    "    \n",
    "#     Here we will concatenate our lexical features and our preprocessed features\n",
    "    r = get_final_data(r, data)\n",
    "    if log :\n",
    "        print(\"Padded The Lexical And Preprocessed Features of Data\")\n",
    "    \n",
    "    \n",
    "    res = r\n",
    "    \n",
    "    if add_all :\n",
    "        \n",
    "        if log :\n",
    "            print(\"Computing How Many Empty Line To Add To Codes \")\n",
    "        if pad2 is None :\n",
    "            \n",
    "            mean = 0\n",
    "            max_num = -1\n",
    "            for i in r :\n",
    "                mean += i.shape[0]\n",
    "                max_num = max(max_num, i.shape[0])\n",
    "                \n",
    "                \n",
    "            mean /= r.shape[0]\n",
    "            nums = [int(mean), max_num]\n",
    "            pad2 = nums[type_add]\n",
    "            \n",
    "        res = []\n",
    "        if log :\n",
    "            print(\"Computed How Many Empty Line To Add To Codes \")\n",
    "        for i in r :\n",
    "            \n",
    "            if i.shape[0] < pad2 :\n",
    "                \n",
    "                zeros = np.zeros([pad2 - i.shape[0], i.shape[1]])\n",
    "                zeros[:, -2] = 1\n",
    "                temp = np.concatenate([i, zeros], axis = 0)\n",
    "            elif i.shape[0] > pad2 :\n",
    "                temp = i[:pad2, :]\n",
    "            else :\n",
    "                temp = i\n",
    "            res.append(temp)\n",
    "        if log :\n",
    "            print(\"Added All The Empty Lines \")\n",
    "\n",
    "    res = np.array(res)\n",
    "    \n",
    "    save_r = r.copy()\n",
    "    \n",
    "    r = np.concatenate(res, axis = 0).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    if scaler is None :\n",
    "        \n",
    "        scaler = StandardScaler().fit(r[:, :-2].astype(np.float32))\n",
    "        if log :\n",
    "            print(\"Computed Mean And Standard Deviation For Normalization \")\n",
    "    \n",
    "    for i, iv in enumerate(res) :\n",
    "        \n",
    "        res[i, :, :-2] = scaler.transform(iv[:, :-2].astype(np.float32)).astype(np.float32)\n",
    "        \n",
    "        if log :\n",
    "            print(\"Data Was Normalized \")\n",
    "\n",
    "    \n",
    "    if return_before_pad :\n",
    "        return res, scaler, pad1, pad2, save_r\n",
    "        \n",
    "    return res, scaler, pad1, pad2\n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                 \n",
    "                \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making The Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(shape):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    gets the first rnn model\n",
    "    shape : shape of the input : shape of the codes [Number of lines (which can be None), Number of Fetures per line]\n",
    "    '''\n",
    "    in1 = Input(shape)\n",
    "    X = Bidirectional(LSTM(150, return_sequences=True, dropout=0.25, recurrent_dropout=0.1))(in1)\n",
    "    X = LSTM(150, return_sequences=True, dropout=0.25, recurrent_dropout=0.1,)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Dense(256, activation=relu)(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(128, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.25)(X)\n",
    "    X = Dense(64, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.3)(X)\n",
    "    X = Dense(32, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    X = Dense(16, activation=relu)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    \n",
    "    X = Dense(2, activation=softmax)(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Model(in1, X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Measurements For Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(y_true, y_pred):\n",
    "    \n",
    "    acc = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "#     True Positive\n",
    "    tp =  acc[1][1]\n",
    "#     False Negative\n",
    "    fn =  acc[1][0]\n",
    "#     False Positive\n",
    "    fp =  acc[0][1]\n",
    "#     True Negative\n",
    "    tn =  acc[0][0]\n",
    "#     Recall\n",
    "    rec1 = acc[1][1] / (acc[1][1] + acc[1][0])\n",
    "#     Precision \n",
    "    prec1 = acc[1][1] / (acc[1][1] + acc[0][1])\n",
    "#     Over ALl Accuracy\n",
    "    accuracy = (acc[1][1] + acc[0][0]) / (acc[1][1] + acc[0][0] + acc[1][0] + acc[0][1])\n",
    "#     F1 Accuracy\n",
    "    f1 = 2.0 / ((1.0/rec1) + (1.0/prec1))\n",
    "    \n",
    "    return rec1, prec1, accuracy, f1 , tp , fn , fp , tn\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Paring Data For Measurements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mus(y_true, x, model):\n",
    "    \n",
    "    \n",
    "    y_t = np.argmax(y_true, axis = -1).reshape((-1))\n",
    "    y_p = model.predict(x)\n",
    "    y_p = np.argmax(y_p, axis=-1).reshape((-1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return get_acc(y_t, y_p)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using All The Defined Functions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K is for our K-fold\n",
    "k = 10\n",
    "# The name of the codes we want to use, you can slice this list to a smaller list for a fast test\n",
    "l = list_files[:]\n",
    "# The number of codes we use on each fold\n",
    "size = math.ceil(len(l) / k)\n",
    "# Verbose for our NN model \n",
    "verbose = 0 \n",
    "\n",
    "# The results for trains and tests respectivly\n",
    "trs = []\n",
    "ts = []\n",
    "\n",
    "for i in range(k):\n",
    "    \n",
    "    print(\"k\", i)\n",
    "#     start and end will be the indicies for what we'll use for test\n",
    "    start  = i * size\n",
    "    end    = min(len(l), (i + 1) * size)\n",
    "    \n",
    "    data_train = l[:start] + l[end:]\n",
    "    data_test  = l[start : end]\n",
    "    \n",
    "    if len(data_test) <= 0 or len(data_train) <= 0 :\n",
    "        print(\"hey\")\n",
    "        continue\n",
    "\n",
    "   \n",
    "    # gathering data for train\n",
    "    r_train, scaler, pad1, pad2 = gather_data(data_train, archive, add_all = True)\n",
    "    # gathering data for test, please note that the same mean and standard deviation that was computed for train will be used to \n",
    "    # normalize test data and also the information of pad1 and pad2 is computed from train so that no information will be \n",
    "    # leaked from train and also none of the aforementioned are dependent on the test data \n",
    "    r_test, _, _, _ = gather_data(data_test, archive, scaler = scaler, add_all = True, pad1 = pad1, pad2 = pad2)\n",
    "    \n",
    "    print(\"data read\")\n",
    "    \n",
    "    # configuring model\n",
    "    model = get_model([None, r_train.shape[-1] - 2])\n",
    "    loss = get_loss_function(WEIGHTS_FOR_LOSS)\n",
    "    model.compile(keras.optimizers.adam(lr = 1e-3), keras.losses.categorical_crossentropy, metrics = [\"accuracy\"])\n",
    "\n",
    "    \n",
    "    # making the X, y for train and test set\n",
    "    X_train = r_train[:, :, :-2]\n",
    "    y_train = r_train[:, :, -2:]\n",
    "    \n",
    "    \n",
    "    X_test = r_test[:, :, :-2]\n",
    "    y_test = r_test[:, :, -2:]\n",
    "    \n",
    "    # training the model\n",
    "    print(\"training on the data started \")\n",
    "    model.fit(X_train, y_train, validation_data = [X_test, y_test], epochs = 20, batch_size = 8, verbose = verbose)\n",
    "    print(\"training on the data finished \")\n",
    "    \n",
    "    # saving and printing the accuracy of training data\n",
    "    print(\"train : rec1, prec1, accuracy, f1, acc , tp , fn , fp , tn \")\n",
    "    trs.append(get_mus(y_train, X_train,model))\n",
    "    print(trs[-1])\n",
    "\n",
    "    # preparing to write the training accuracy to a file\n",
    "    strRes = \"train : \"\n",
    "    for counter in range(8):\n",
    "        strRes = strRes + '%.5f' % trs[-1][counter] + \" , \"\n",
    "    \n",
    "    strRes += \" \\n \"\n",
    "    \n",
    "\n",
    "    # saving and printing the accuracy of testing data\n",
    "    print(\"test : rec1, prec1, accuracy, f1, acc, tp , fn , fp , tn \")\n",
    "    ts.append(get_mus(y_test, X_test,model))\n",
    "    print(ts[-1])\n",
    "    \n",
    "    # preparing to write the testing accuracy to a file\n",
    "    strRes += \" test :  \"\n",
    "    for counter in range(8):\n",
    "        strRes = strRes + '%.5f' % ts[-1][counter] + \" , \"\n",
    "    \n",
    "    # writing the accuracies on to a file\n",
    "    f = open(\"test.txt\", \"a\")\n",
    "    f.write(strRes + \"\\n\")\n",
    "    f.close()\n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
